{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae5e8e1",
   "metadata": {},
   "source": [
    "# NLP Disasters Mini Project\n",
    "**Course:** CSCA 5642\n",
    "**Date:** Dec 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c00f0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os, re\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" # Force CPU to avoid CUDA 12.0 errors with RTX 5090\n",
    "\n",
    "# Plotting Libs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP Libs\n",
    "import nltk\n",
    "# NLTK One-Time Downloads\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Embedding Libs\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "# Linear Model Libs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "# Neural Network Libs\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c715f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "punct_table = str.maketrans('', '', string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_pos_lemma(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text.lower())\n",
    "    tokens = text.translate(punct_table).split()\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    return ' '.join(lemmatizer.lemmatize(w, pos=wordnet.ADJ if t.startswith('J')\n",
    "                                         else wordnet.VERB if t.startswith('V')\n",
    "                                         else wordnet.NOUN if t.startswith('N')\n",
    "                                         else wordnet.ADV if t.startswith('R')\n",
    "                                         else wordnet.NOUN)\n",
    "                    for w, t in tagged)\n",
    "\n",
    "def clean_light(text):\n",
    "    # minimal cleaning; keeps punctuation/hashtags/mentions for n-grams\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text.lower())\n",
    "\n",
    "def clean_char_preserve(text):\n",
    "    # for char n-grams; almost no cleaning\n",
    "    return text.lower()\n",
    "\n",
    "CLEANERS = {\n",
    "    'pos_lemma': clean_pos_lemma,\n",
    "    'light': clean_light,\n",
    "    'char_keep': clean_char_preserve,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7621bb66",
   "metadata": {},
   "source": [
    "## Data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58fa6bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterData:\n",
    "    def __init__(self, path: str, val_size=0.2, random_state=11):\n",
    "        full_train = pd.read_csv(f'{path}/train.csv')\n",
    "        self.test_df = pd.read_csv(f'{path}/test.csv')\n",
    "        self.train_df, self.val_df = train_test_split(\n",
    "            full_train, test_size=val_size, random_state=random_state, stratify=full_train['target']\n",
    "        )\n",
    "        self._clean_cache = {} \n",
    "\n",
    "    def get_split(self, cleaner_name: str):\n",
    "        \"\"\"Return cleaned splits for the requested cleaner; caches to avoid recompute.\"\"\"\n",
    "        if cleaner_name not in CLEANERS:\n",
    "            raise ValueError(f'Unknown cleaner {cleaner_name}')\n",
    "        if cleaner_name not in self._clean_cache:\n",
    "            fn = CLEANERS[cleaner_name]\n",
    "            self._clean_cache[cleaner_name] = (\n",
    "                self.train_df['text'].apply(fn),\n",
    "                self.val_df['text'].apply(fn),\n",
    "                self.test_df['text'].apply(fn),\n",
    "            )\n",
    "        X_train, X_val, X_test = self._clean_cache[cleaner_name]\n",
    "        return {\n",
    "            'X_train': X_train,\n",
    "            'y_train': self.train_df['target'],\n",
    "            'X_val': X_val,\n",
    "            'y_val': self.val_df['target'],\n",
    "            'X_test': X_test,\n",
    "            'test_ids': self.test_df['id'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293138cc",
   "metadata": {},
   "source": [
    "## TF-IDF Preprocessing + LogReg Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f97453a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline TF-IDF + LogReg F1 Score: 0.7786\n"
     ]
    }
   ],
   "source": [
    "# Baseline model runner\n",
    "def run_tfidf_logreg(data, tfidf_cfg, logreg_cfg, threshold=None):\n",
    "    vec = TfidfVectorizer(**tfidf_cfg)\n",
    "    Xtr = vec.fit_transform(data['X_train'])\n",
    "    Xval = vec.transform(data['X_val'])\n",
    "\n",
    "    model = LogisticRegression(**logreg_cfg)\n",
    "    model.fit(Xtr, data['y_train'])\n",
    "\n",
    "    val_probs = model.predict_proba(Xval)[:, 1]\n",
    "    if threshold is None:\n",
    "        grid = np.linspace(0.3, 0.7, 11)\n",
    "        f1s = [f1_score(data['y_val'], (val_probs >= t).astype(int)) for t in grid]\n",
    "        threshold = grid[int(np.argmax(f1s))]\n",
    "    val_pred = (val_probs >= threshold).astype(int)\n",
    "    val_f1 = f1_score(data['y_val'], val_pred)\n",
    "    return {'val_f1': val_f1, 'threshold': threshold, 'model': model, 'vectorizer': vec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment config and grid\n",
    "base = {\n",
    "    'cleaner': 'light',\n",
    "    'tfidf': {'max_features': 20000, 'ngram_range': (1,2), 'analyzer': 'word'},\n",
    "    'logreg': {'C': 2.0, 'penalty': 'l2', 'max_iter': 1000, 'class_weight': 'balanced', 'n_jobs': -1},\n",
    "}\n",
    "grid = [\n",
    "    base,\n",
    "    {**base, 'cleaner': 'char_keep', 'tfidf': {**base['tfidf'], 'analyzer': 'char', 'ngram_range': (3,5)}},\n",
    "    {**base, 'cleaner': 'pos_lemma'},\n",
    "]\n",
    "\n",
    "disaster_data = DisasterData(path='data')\n",
    "results = []\n",
    "artifacts = {}\n",
    "for cfg in grid:\n",
    "    split = disaster_data.get_split(cfg['cleaner'])\n",
    "    res = run_tfidf_logreg(split, cfg['tfidf'], cfg['logreg'])\n",
    "    name = f\"{cfg['cleaner']}_{cfg['tfidf']['analyzer']}_{cfg['tfidf']['ngram_range']}\"\n",
    "    results.append({'name': name, 'val_f1': res['val_f1'], 'threshold': res['threshold']})\n",
    "    artifacts[name] = (res, split, cfg)\n",
    "\n",
    "pd.DataFrame(results).sort_values('val_f1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971675ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = pd.DataFrame(results).sort_values('val_f1', ascending=False).iloc[0]['name']\n",
    "res, split, cfg = artifacts[best_name]\n",
    "\n",
    "# Refit on full train (train+val) with same cleaner/vectorizer settings\n",
    "full_clean = pd.concat([split['X_train'], split['X_val']])\n",
    "full_y = pd.concat([split['y_train'], split['y_val']])\n",
    "\n",
    "vec = TfidfVectorizer(**cfg['tfidf'])\n",
    "Xfull = vec.fit_transform(full_clean)\n",
    "model = LogisticRegression(**cfg['logreg'])\n",
    "model.fit(Xfull, full_y)\n",
    "\n",
    "Xtest = vec.transform(split['X_test'])\n",
    "test_pred = (model.predict_proba(Xtest)[:,1] >= res['threshold']).astype(int)\n",
    "\n",
    "print(f'Best model: {best_name}')\n",
    "print(f'F1 Score: {res[\"val_f1\"]:.4f}')\n",
    "print(f'Threshold: {res[\"threshold\"]:.4f}')\n",
    "\n",
    "# No baseline submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd91d2d",
   "metadata": {},
   "source": [
    "## RNN Model (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a7a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buffalo/miniforge3/envs/school/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=20000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(disaster_data.X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(disaster_data.X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(disaster_data.X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(disaster_data.X_test)\n",
    "\n",
    "maxlen = 40\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, padding='post', maxlen=maxlen)\n",
    "X_val_pad = pad_sequences(X_val_seq, padding='post', maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test_seq, padding='post', maxlen=maxlen)\n",
    "\n",
    "vocab_size = min(20000, len(tokenizer.word_index) + 1)\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfbc789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5703 - loss: 0.6845 - val_accuracy: 0.5706 - val_loss: 0.6855\n",
      "Epoch 2/3\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6169 - loss: 0.6570 - val_accuracy: 0.7498 - val_loss: 0.5451\n",
      "Epoch 3/3\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6969 - loss: 0.5637 - val_accuracy: 0.5673 - val_loss: 0.6588\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "LSTM F1 Score: 0.6451\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train_pad, disaster_data.y_train,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val_pad, disaster_data.y_val),\n",
    "    )\n",
    "\n",
    "val_pred_prob = model.predict(X_val_pad).ravel()\n",
    "val_pred = (val_pred_prob >= 0.5).astype(int)\n",
    "val_f1_rnn = f1_score(disaster_data.y_val, val_pred)\n",
    "print(f'LSTM F1 Score: {val_f1_rnn:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2793976a",
   "metadata": {},
   "source": [
    "### Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd67103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "test_pred_prob = model.predict(X_test_pad).ravel()\n",
    "test_pred = (test_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "disaster_data.submission = pd.DataFrame({\n",
    "    'id': disaster_data.test_df.id,\n",
    "    'target': test_pred\n",
    "})\n",
    "disaster_data.submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4f129",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c442ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Misclassified Text ---\n",
      "Text: @Blizzard_draco @LoneWolffur I need this.\n",
      "Pred: 1\n",
      "True: 0\n",
      "Nearest Neighbors:\n",
      "Text: @Blizzard_draco @LoneWolffur also me please I would very much like a link\n",
      "Text: @LoneWolffur control yourself tora\n",
      "Text: @LoneWolffur BRUH *dies*\n",
      "\n",
      "--- Misclassified Text ---\n",
      "Text: #news Politifiact: Harry Reid's '30 Percent of Women Served' Planned Parenthood Claim Is a 'Pants on Fire' Lie... http://t.co/bMSeDZOfSV\n",
      "Pred: 1\n",
      "True: 0\n",
      "Nearest Neighbors:\n",
      "Text: Politifiact: Harry Reid's '30 Percent of Women Served' Planned Parenthood Claim Is a 'Pants on Fire' Lie http://t.co/aMYMwWcpYm | #tcot\n",
      "Text: 30 seconds for my bitches to evacuate ??????\n",
      "Text: @NoahCRothman Bore him with minutiae serve bad champagne. He may just explode.\n",
      "\n",
      "--- Misclassified Text ---\n",
      "Text: @mustachemurse @dateswhitecoats the truth. I pulled a 16 out. And apparently a 22 in the crazy adult trauma. And they mocked me for the 22.\n",
      "Pred: 1\n",
      "True: 0\n",
      "Nearest Neighbors:\n",
      "Text: SCREAMING IN 22 DIFFERENT LANGUAGES http://t.co/rDfaAKKbNJ\n",
      "Text: Dead Space - Obliteration Imminent [2/2]: http://t.co/XJB0dCAaHf via @YouTube\n",
      "Text: into burning fucking buildings (2/2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "mis_idx = np.where(val_pred != disaster_data.y_val)[0][:3]\n",
    "\n",
    "for i in mis_idx:\n",
    "    text_i = disaster_data.X_val_raw.iloc[i]\n",
    "    vec_i = X_val_tfidf[i]\n",
    "\n",
    "    # find nearest neighbors in training set\n",
    "    sims = cosine_similarity(vec_i, X_train_tfidf).ravel()\n",
    "    top_idx = sims.argsort()[-3:][::-1]\n",
    "\n",
    "    print('--- Misclassified Text ---')\n",
    "    print('Text:', text_i)\n",
    "    print('Pred:', val_pred[i])\n",
    "    print('True:', disaster_data.y_val.iloc[i])\n",
    "    print('Nearest Neighbors:')\n",
    "    for idx in top_idx:\n",
    "        print(f'Text: {disaster_data.X_train_raw.iloc[idx]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09217dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
